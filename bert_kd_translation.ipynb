{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6458418e",
   "metadata": {},
   "source": [
    "# BERT Knowledge Distillation for Translation\n",
    "\n",
    "This notebook implements the three-stage approach from the paper [\"Distilling Knowledge Learned in BERT for Text Generation\"](https://arxiv.org/abs/1911.03829) (ACL 2020):\n",
    "\n",
    "1. **CMLM Finetuning**: Fine-tune BERT as a conditional masked language model on translation data\n",
    "2. **Knowledge Extraction**: Extract hidden states and compute teacher logits\n",
    "3. **Student Training**: Train a smaller encoder-decoder translation model with knowledge distillation\n",
    "   - Approach 1: Using OpenNMT's Transformer (standard approach from the paper)\n",
    "   - Approach 2: Using Hugging Face's T5 model (alternative implementation)\n",
    "\n",
    "We'll work with the IWSLT14 German-English dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad931c70",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "\n",
    "First, we'll install the required dependencies and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8d11da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers==4.26.0\n",
    "!pip install pytorch-pretrained-bert\n",
    "!pip install cytoolz\n",
    "!pip install tqdm\n",
    "!pip install shelve-utils\n",
    "\n",
    "# Import common libraries\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import shelve\n",
    "import io\n",
    "import argparse\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "\n",
    "# Add project directories to path for importing modules\n",
    "sys.path.append('.')\n",
    "sys.path.append('./opennmt')\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeea0bbb",
   "metadata": {},
   "source": [
    "## Download and Preprocess the IWSLT14 Dataset\n",
    "\n",
    "We'll download the German-English translation dataset and prepare it for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6653ac46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories for data and outputs\n",
    "!mkdir -p data/\n",
    "!mkdir -p output/cmlm_model\n",
    "!mkdir -p output/bert_dump\n",
    "!mkdir -p output/kd-model/ckpt\n",
    "!mkdir -p output/kd-model/log\n",
    "!mkdir -p output/translation\n",
    "\n",
    "# Download IWSLT German-English dataset using the provided script\n",
    "!bash scripts/download-iwslt_deen.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be62e15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset using the provided script\n",
    "!bash scripts/prepare-iwslt_deen.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbefae33",
   "metadata": {},
   "source": [
    "## Apply BERT Tokenization\n",
    "\n",
    "We need to tokenize our data with the BERT tokenizer for the CMLM finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8c4bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.bert_tokenize import tokenize, process\n",
    "\n",
    "# Load BERT tokenizer\n",
    "bert_model = \"bert-base-multilingual-cased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model, do_lower_case='uncased' in bert_model)\n",
    "\n",
    "# Define data directories\n",
    "data_dir = \"data/de-en\"\n",
    "\n",
    "# BERT tokenize our dataset files\n",
    "for language in ['de', 'en']:\n",
    "    for split in ['train', 'valid', 'test']:\n",
    "        input_file = f\"{data_dir}/{split}.{language}\"\n",
    "        output_file = f\"{data_dir}/{split}.{language}.bert\"\n",
    "        print(f\"Tokenizing {input_file}...\")\n",
    "        \n",
    "        with open(input_file, 'r') as reader, open(output_file, 'w') as writer:\n",
    "            process(reader, writer, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66af4716",
   "metadata": {},
   "source": [
    "## Prepare BERT Training Data\n",
    "\n",
    "Now we'll prepare the database and vocabulary for BERT finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a619e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset DB for BERT training\n",
    "from scripts.bert_prepro import main as bert_prepro\n",
    "\n",
    "# Set up args for bert_prepro\n",
    "prepro_args = argparse.Namespace(\n",
    "    src=f\"{data_dir}/train.de.bert\",\n",
    "    tgt=f\"{data_dir}/train.en.bert\",\n",
    "    output='data/DEEN.db'\n",
    ")\n",
    "\n",
    "# Run preprocessing\n",
    "bert_prepro(prepro_args)\n",
    "\n",
    "# Create vocabulary file using OpenNMT's preprocess.py\n",
    "!python opennmt/preprocess.py \\\n",
    "    -train_src {data_dir}/train.de.bert \\\n",
    "    -train_tgt {data_dir}/train.en.bert \\\n",
    "    -valid_src {data_dir}/valid.de.bert \\\n",
    "    -valid_tgt {data_dir}/valid.en.bert \\\n",
    "    -save_data data/DEEN \\\n",
    "    -src_seq_length 150 -tgt_seq_length 150"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c72ac6",
   "metadata": {},
   "source": [
    "## Stage 1: CMLM (Conditional Masked Language Model) Finetuning\n",
    "\n",
    "In this stage, we fine-tune BERT as a Conditional Masked Language Model on our translation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261c988b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert.optimization import BertAdam\n",
    "\n",
    "# Import needed modules\n",
    "from cmlm.data import BertDataset, TokenBucketSampler\n",
    "from cmlm.model import convert_embedding, BertForSeq2seq\n",
    "from cmlm.util import Logger, RunningMeter\n",
    "from run_cmlm_finetuning import noam_schedule\n",
    "\n",
    "# Load vocabulary using our compatibility module\n",
    "from vocab_loader import safe_load_vocab\n",
    "\n",
    "vocab_file = \"data/DEEN.vocab.pt\"\n",
    "train_file = \"data/DEEN.db\"\n",
    "valid_src = f\"{data_dir}/valid.de.bert\"\n",
    "valid_tgt = f\"{data_dir}/valid.en.bert\"\n",
    "output_dir = \"output/cmlm_model\"\n",
    "\n",
    "# Load vocabulary using custom loader to avoid PyTorch compatibility issues\n",
    "vocab_dump = safe_load_vocab(vocab_file)\n",
    "vocab = vocab_dump['tgt'].fields[0][1].vocab.stoi\n",
    "\n",
    "# Create dataset\n",
    "train_dataset = BertDataset(train_file, tokenizer, vocab, seq_len=512, max_len=150)\n",
    "\n",
    "# Define sampler and data loader\n",
    "BUCKET_SIZE = 8192\n",
    "train_sampler = TokenBucketSampler(\n",
    "    train_dataset.lens, BUCKET_SIZE, 6144, batch_multiple=1)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_sampler=train_sampler,\n",
    "                         num_workers=4,\n",
    "                         collate_fn=BertDataset.pad_collate)\n",
    "\n",
    "# Prepare model\n",
    "model = BertForSeq2seq.from_pretrained(bert_model)\n",
    "embedding = convert_embedding(tokenizer, vocab, model.bert.embeddings.word_embeddings.weight)\n",
    "model.update_output_layer(embedding)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3157f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "learning_rate = 5e-5\n",
    "warmup_proportion = 0.1  # Using proportion instead of absolute steps\n",
    "max_steps = 100000  # Full training uses 100k steps\n",
    "num_steps_to_run = 5000  # We'll do fewer steps for demonstration\n",
    "\n",
    "# Optimizer\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer\n",
    "                if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer\n",
    "                if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                    lr=learning_rate,\n",
    "                    warmup=warmup_proportion,\n",
    "                    t_total=max_steps)\n",
    "\n",
    "# Training loop\n",
    "running_loss = RunningMeter('loss')\n",
    "model.train()\n",
    "\n",
    "print(\"Starting CMLM fine-tuning...\")\n",
    "# Use a plain iterator instead of tqdm with len()\n",
    "train_iter = iter(train_loader)\n",
    "for step in range(num_steps_to_run):\n",
    "    try:\n",
    "        batch = next(train_iter)\n",
    "    except StopIteration:\n",
    "        # Restart iterator if we run out of batches\n",
    "        train_iter = iter(train_loader)\n",
    "        batch = next(train_iter)\n",
    "        \n",
    "    # Move batch to device\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    input_ids, input_mask, segment_ids, lm_label_ids = batch\n",
    "    \n",
    "    # Zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    loss = model(input_ids, segment_ids, input_mask, lm_label_ids)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    running_loss(loss.item())\n",
    "    \n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step}, Loss: {running_loss.val:.4f}\")\n",
    "\n",
    "# Save model checkpoint\n",
    "torch.save(model.state_dict(), f\"{output_dir}/model_step_{num_steps_to_run}.pt\")\n",
    "print(f\"Model saved to {output_dir}/model_step_{num_steps_to_run}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4981bfac",
   "metadata": {},
   "source": [
    "## Stage 2: Extract Knowledge from Teacher Model\n",
    "\n",
    "Now we'll extract the hidden states from our fine-tuned BERT model and compute the top-k logits that will be used for knowledge distillation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebefcaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import extraction functions\n",
    "from dump_teacher_hiddens import tensor_dumps, gather_hiddens, BertSampleDataset, batch_features, process_batch\n",
    "\n",
    "# Path to model checkpoint from Stage 1\n",
    "ckpt_path = f\"{output_dir}/model_step_{num_steps_to_run}.pt\"\n",
    "bert_dump_path = \"output/bert_dump\"\n",
    "\n",
    "# Load the fine-tuned BERT model\n",
    "state_dict = torch.load(ckpt_path)\n",
    "vsize = state_dict['cls.predictions.decoder.weight'].size(0)\n",
    "bert = BertForSeq2seq.from_pretrained(bert_model).eval()\n",
    "bert.to(device)\n",
    "\n",
    "# Update output layer\n",
    "bert.update_output_layer_by_size(vsize)\n",
    "bert.load_state_dict(state_dict)\n",
    "\n",
    "# Save the final projection layer\n",
    "linear = torch.nn.Linear(bert.config.hidden_size, bert.config.vocab_size)\n",
    "linear.weight.data = state_dict['cls.predictions.decoder.weight']\n",
    "linear.bias.data = state_dict['cls.predictions.bias']\n",
    "torch.save(linear, f'{bert_dump_path}/linear.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c58071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract hidden states\n",
    "def build_db_batched(corpus_path, out_db, bert, toker, batch_size=8):\n",
    "    dataset = BertSampleDataset(corpus_path, toker)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size,\n",
    "                       num_workers=4, collate_fn=batch_features)\n",
    "    \n",
    "    with tqdm(desc='Computing BERT features', total=len(dataset)) as pbar:\n",
    "        for ids, *batch in loader:\n",
    "            outputs = process_batch(batch, bert, toker)\n",
    "            for id_, output in zip(ids, outputs):\n",
    "                if output is not None:\n",
    "                    out_db[id_] = tensor_dumps(output)\n",
    "            pbar.update(len(ids))\n",
    "\n",
    "# Extract hidden states\n",
    "db_path = \"data/DEEN.db\"\n",
    "print(\"Extracting hidden states...\")\n",
    "with shelve.open(f'{bert_dump_path}/db', 'c') as out_db, torch.no_grad():\n",
    "    build_db_batched(db_path, out_db, bert, tokenizer, batch_size=8)\n",
    "\n",
    "print(f\"Hidden states extracted and saved to {bert_dump_path}/db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f31547",
   "metadata": {},
   "source": [
    "## Computing Top-K Logits\n",
    "\n",
    "Now we'll compute the top-k logits from the extracted hidden states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab6b179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions for top-k computation\n",
    "from dump_teacher_topk import tensor_loads, dump_topk\n",
    "\n",
    "# Top-K parameter\n",
    "k = 8  # Following the paper\n",
    "\n",
    "# Load linear layer\n",
    "linear = torch.load(f'{bert_dump_path}/linear.pt')\n",
    "linear.to(device)\n",
    "\n",
    "# Compute top-k logits\n",
    "print(\"Computing top-k logits...\")\n",
    "with shelve.open(f'{bert_dump_path}/db', 'r') as db, \\\n",
    "     shelve.open(f'{bert_dump_path}/topk', 'c') as topk_db:\n",
    "    for key, value in tqdm(db.items(), total=len(db), desc='Computing topk...'):\n",
    "        bert_hidden = torch.tensor(tensor_loads(value)).to(device)\n",
    "        topk = linear(bert_hidden).topk(dim=-1, k=k)\n",
    "        dump = dump_topk(topk)\n",
    "        topk_db[key] = dump\n",
    "\n",
    "print(f\"Top-k logits computed and saved to {bert_dump_path}/topk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9f07c7",
   "metadata": {},
   "source": [
    "## Stage 3: Train Student Translation Model with Knowledge Distillation\n",
    "\n",
    "We'll implement two different approaches for the student model:\n",
    "1. **Approach 1: OpenNMT Transformer** - The standard approach from the paper\n",
    "2. **Approach 2: Hugging Face T5 Model** - An alternative implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb31db20",
   "metadata": {},
   "source": [
    "### Approach 1: OpenNMT Transformer\n",
    "\n",
    "This is the approach used in the original paper implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc88751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules for training\n",
    "from onmt.inputters.bert_kd_dataset import BertKdDataset, TokenBucketSampler\n",
    "from onmt.utils.optimizers import Optimizer\n",
    "from onmt.train_single import build_model_saver, build_trainer, cycle_loader\n",
    "\n",
    "# Define paths\n",
    "data_db = \"data/DEEN.db\"\n",
    "bert_dump = \"output/bert_dump\"\n",
    "data = \"data/DEEN\"\n",
    "config_path = \"opennmt/config/config-transformer-base-mt-deen.yml\"\n",
    "output_path = \"output/kd-model\"\n",
    "\n",
    "# Load configuration\n",
    "with open(config_path, 'r') as stream:\n",
    "    config = yaml.safe_load(stream)\n",
    "\n",
    "# Create args object\n",
    "args = argparse.Namespace(**config)\n",
    "\n",
    "# Setup KD parameters\n",
    "args.train_from = None\n",
    "args.max_grad_norm = None\n",
    "args.kd_topk = 8\n",
    "args.train_steps = 100000\n",
    "args.kd_temperature = 10.0\n",
    "args.kd_alpha = 0.5\n",
    "args.warmup_steps = 8000\n",
    "args.learning_rate = 2.0\n",
    "args.bert_dump = bert_dump\n",
    "args.data_db = data_db\n",
    "args.bert_kd = True\n",
    "args.data = data\n",
    "\n",
    "args.save_model = os.path.join(output_path, 'ckpt', 'model')\n",
    "args.log_file = os.path.join(output_path, 'log', 'log')\n",
    "args.tensorboard_log_dir = os.path.join(output_path, 'log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8247096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vocabulary and dataset\n",
    "vocab = torch.load(data + '.vocab.pt')\n",
    "src_vocab = vocab['src'].fields[0][1].vocab.stoi\n",
    "tgt_vocab = vocab['tgt'].fields[0][1].vocab.stoi\n",
    "\n",
    "# Create dataset\n",
    "train_dataset = BertKdDataset(data_db, bert_dump, \n",
    "                             src_vocab, tgt_vocab,\n",
    "                             max_len=150, k=args.kd_topk)\n",
    "\n",
    "# Create data loader\n",
    "BUCKET_SIZE = 8192\n",
    "train_sampler = TokenBucketSampler(\n",
    "    train_dataset.keys, BUCKET_SIZE, 6144,\n",
    "    batch_multiple=1)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_sampler=train_sampler,\n",
    "                         num_workers=4,\n",
    "                         collate_fn=BertKdDataset.pad_collate)\n",
    "\n",
    "train_iter = cycle_loader(train_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87617e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "from onmt.model_builder import build_model\n",
    "model = build_model(args, args, checkpoint=None)\n",
    "model.to(device)\n",
    "\n",
    "# Build optimizer\n",
    "optim = Optimizer.from_opt(model, args, checkpoint=None)\n",
    "\n",
    "# Build model saver\n",
    "model_saver = build_model_saver(args, args, model, vocab, optim)\n",
    "\n",
    "# Build trainer\n",
    "trainer = build_trainer(args, 0, model, vocab, optim, model_saver=model_saver)\n",
    "\n",
    "# Train - for demonstration, we'll only do a few steps\n",
    "num_steps_to_run_kd = 500  # Adjust for full training (paper used 100k steps)\n",
    "\n",
    "print(\"Starting model training with knowledge distillation...\")\n",
    "trainer.train(\n",
    "    train_iter,\n",
    "    num_steps_to_run_kd,\n",
    "    valid_iter=None\n",
    ")\n",
    "\n",
    "print(f\"Model trained for {num_steps_to_run_kd} steps and saved to {output_path}/ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f93fae",
   "metadata": {},
   "source": [
    "### Approach 2: Hugging Face T5 Model\n",
    "\n",
    "This is an alternative implementation using Hugging Face's T5 model. T5 is a powerful encoder-decoder transformer model that can be fine-tuned for translation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67abc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules for Hugging Face approach\n",
    "!pip install transformers==4.26.0 datasets\n",
    "from transformers import T5Config, T5ForConditionalGeneration, TrainingArguments\n",
    "from onmt.KD_training.dataset import BertKdDataset\n",
    "from onmt.KD_training.trainer import BertKDTrainer\n",
    "\n",
    "# Define paths\n",
    "t5_output_path = \"output/kd-model-t5\"\n",
    "\n",
    "# Make output directories\n",
    "!mkdir -p {t5_output_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c0f682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset is already loaded from the previous approach\n",
    "# We can reuse the same BertKdDataset from before\n",
    "\n",
    "# Load vocabulary and dataset for the T5 approach\n",
    "vocab = torch.load(data + '.vocab.pt')\n",
    "src_vocab = vocab['src'].fields[0][1].vocab.stoi\n",
    "tgt_vocab = vocab['tgt'].fields[0][1].vocab.stoi\n",
    "\n",
    "# Create dataset (same as before)\n",
    "t5_train_dataset = BertKdDataset(data_db, bert_dump, \n",
    "                             src_vocab, tgt_vocab,\n",
    "                             max_len=150, k=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db6d40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the T5 model with configuration matching the paper specifications\n",
    "# Get vocabulary size from dataset to avoid dimension mismatch\n",
    "vocab_size = len(tgt_vocab)\n",
    "# Ensure vocabulary size is a multiple of 8 for tensor cores efficiency\n",
    "if vocab_size % 8 != 0:\n",
    "    vocab_size += (8 - vocab_size % 8)\n",
    "\n",
    "print(f\"Using vocabulary size: {vocab_size}\")\n",
    "\n",
    "distill_config = T5Config(\n",
    "    vocab_size=vocab_size,  # Use dataset vocabulary size instead of default\n",
    "    d_model=512,       # Hidden size\n",
    "    d_kv=64,           # Size of key/value projections\n",
    "    d_ff=2048,         # Feed-forward intermediate size\n",
    "    num_layers=6,      # Number of encoder layers\n",
    "    num_decoder_layers=6, # Number of decoder layers\n",
    "    num_heads=8,       # Number of attention heads\n",
    "    dropout_rate=0.3,  # Dropout rate as specified in the paper\n",
    "    layer_norm_epsilon=1e-06,\n",
    "    initializer_factor=1.0, \n",
    "    feed_forward_proj='relu',\n",
    "    is_encoder_decoder=True,  # This is a seq2seq model\n",
    "    use_cache=True,\n",
    "    pad_token_id=0, \n",
    "    eos_token_id=1,\n",
    "    gradient_checkpointing=False\n",
    ")\n",
    "\n",
    "# Create the model\n",
    "t5_model = T5ForConditionalGeneration(config=distill_config)\n",
    "t5_model = t5_model.to(device)\n",
    "print(\"T5 model initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efee22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments for the T5 model\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=t5_output_path,\n",
    "    evaluation_strategy=\"steps\", \n",
    "    eval_steps=1000,\n",
    "    max_steps=50000,            # Adjust as needed\n",
    "    warmup_steps=4000,          # Following the paper\n",
    "    learning_rate=1.0,          # Initial learning rate for the scheduler\n",
    "    optim='adamw_torch',        # AdamW optimizer\n",
    "    adam_beta1=0.9,            \n",
    "    adam_beta2=0.98,            # Following the paper\n",
    "    gradient_accumulation_steps=1,\n",
    "    per_device_train_batch_size=6144,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=5,         # Keep only the last 5 checkpoints\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea42de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For demonstration, define a simple accuracy metric function\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    # Calculate accuracy only on non-padding tokens\n",
    "    mask = labels != 0  # Assuming 0 is pad token id\n",
    "    accurate = (predictions == labels) & mask\n",
    "    return {\n",
    "        'accuracy': accurate.sum() / mask.sum()\n",
    "    }\n",
    "\n",
    "# Create the custom Knowledge Distillation trainer\n",
    "t5_trainer = BertKDTrainer(\n",
    "    vocab,\n",
    "    model=t5_model,  # The T5 model to be trained\n",
    "    args=training_args,  # Training arguments defined above\n",
    "    train_dataset=t5_train_dataset,  # Training dataset with teacher knowledge\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2de0f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For demonstration purposes, we'll just show the setup without running the full training\n",
    "# In a real scenario, you would run this full training which takes significant time\n",
    "print(\"T5 KD training setup complete.\")\n",
    "print(\"To start training, uncomment and run the following code:\")\n",
    "print(\"\"\"\n",
    "# Start the knowledge distillation training\n",
    "t5_trainer.train()\n",
    "\n",
    "# Save the final model\n",
    "t5_trainer.save_model(output_dir=t5_output_path)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b25a0d",
   "metadata": {},
   "source": [
    "#### Using the T5 Model for Translation\n",
    "\n",
    "After training, we could use the model for translation like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff1e85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code for how to use the trained T5 model for translation (not run in this notebook)\n",
    "def translate_with_t5(input_text, model_path):\n",
    "    # Load the trained model\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Tokenize input text (you would use the appropriate tokenizer)\n",
    "    # input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
    "    \n",
    "    # Generate translation\n",
    "    # outputs = model.generate(\n",
    "    #    input_ids,\n",
    "    #    max_length=150,\n",
    "    #    num_beams=5,\n",
    "    #    length_penalty=0.6,\n",
    "    #    early_stopping=True\n",
    "    # )\n",
    "    \n",
    "    # Decode output ids to text\n",
    "    # translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # return translated_text\n",
    "    \n",
    "    print(\"Translation function defined but not executed. Would need proper tokenizer setup.\")\n",
    "\n",
    "# Call the function (not actually executed)\n",
    "# Example: translate_with_t5(\"Dies ist ein Beispieltext.\", t5_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc85235",
   "metadata": {},
   "source": [
    "## Comparison of Approaches\n",
    "\n",
    "In this notebook, we've implemented two different approaches for the student model:\n",
    "\n",
    "1. **OpenNMT Transformer**:\n",
    "   - Uses the OpenNMT framework with custom training code following the official implementation\n",
    "   - More control over the training process and architecture details\n",
    "   - Closer to the implementation described in the original paper\n",
    "\n",
    "2. **Hugging Face T5 Model**:\n",
    "   - Uses the modern Hugging Face Transformers library with T5 model\n",
    "   - Easier integration with the broader ML ecosystem\n",
    "   - More modern implementation with potential for better performance\n",
    "\n",
    "Both approaches implement the same knowledge distillation principle where the student model learns from both labeled data and the distilled knowledge from the BERT teacher model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e16cd0",
   "metadata": {},
   "source": [
    "## Translation and Evaluation\n",
    "\n",
    "Finally, we'll translate some text using our trained model and evaluate the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd939b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths for translation\n",
    "model_path = f\"{output_path}/ckpt/model_step_{num_steps_to_run_kd}.pt\"\n",
    "src_file = f\"{data_dir}/test.de.bert\"\n",
    "tgt_file = f\"{data_dir}/test.en.bert\"\n",
    "out_dir = \"output/translation\"\n",
    "ref_file = f\"{data_dir}/test.en\"\n",
    "\n",
    "# Run translation if model exists\n",
    "if os.path.exists(model_path):\n",
    "    # Run translation\n",
    "    !python opennmt/translate.py -model {model_path} \\\n",
    "                                -src {src_file} \\\n",
    "                                -tgt {tgt_file} \\\n",
    "                                -output {out_dir}/result.en \\\n",
    "                                -beam_size 5 -alpha 0.6 \\\n",
    "                                -length_penalty wu\n",
    "\n",
    "    # Detokenize output\n",
    "    !python scripts/bert_detokenize.py --file {out_dir}/result.en \\\n",
    "                                      --output_dir {out_dir}\n",
    "\n",
    "    # Evaluate with BLEU\n",
    "    !perl opennmt/tools/multi-bleu.perl {ref_file} \\\n",
    "                                       < {out_dir}/result.en.detok \\\n",
    "                                       > {out_dir}/result.bleu\n",
    "\n",
    "    # Display BLEU score\n",
    "    with open(f\"{out_dir}/result.bleu\", \"r\") as f:\n",
    "        print(f.read())\n",
    "else:\n",
    "    print(f\"Model file {model_path} not found. Skipping translation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f2ff3e",
   "metadata": {},
   "source": [
    "## Visualize Training Results\n",
    "\n",
    "Let's visualize the training progress and compare with the results from the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb07bf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install matplotlib if needed\n",
    "!pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Display the figures from the paper\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Plot CMLM finetuning\n",
    "axes[0].set_title('CMLM Finetuning')\n",
    "img = plt.imread('figures/cmlm-finetuning.png')\n",
    "axes[0].imshow(img)\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Plot translation losses\n",
    "axes[1].set_title('Translation Losses')\n",
    "img = plt.imread('figures/translation-losses.png')\n",
    "axes[1].imshow(img)\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Plot translation accuracy\n",
    "axes[2].set_title('Translation Accuracy')\n",
    "img = plt.imread('figures/translation-accuracy.png')\n",
    "axes[2].imshow(img)\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fc3506",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've implemented the three-stage knowledge distillation process described in the paper \"Distilling Knowledge Learned in BERT for Text Generation\":\n",
    "\n",
    "1. Fine-tuned a BERT model as a Conditional Masked Language Model (CMLM)\n",
    "2. Extracted knowledge from the BERT teacher model and computed top-k logits\n",
    "3. Trained a student translation model with knowledge distillation from BERT\n",
    "\n",
    "For a full implementation with complete results, the model should be trained for many more steps:\n",
    "- CMLM finetuning: 100,000 steps\n",
    "- Student model training: 100,000 steps\n",
    "\n",
    "As shown in the figures, knowledge distillation from BERT can improve translation performance compared to baseline methods."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
