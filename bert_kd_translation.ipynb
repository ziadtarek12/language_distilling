{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6458418e",
   "metadata": {},
   "source": [
    "# BERT Knowledge Distillation for Translation\n",
    "\n",
    "This notebook implements the three-stage approach from the paper [\"Distilling Knowledge Learned in BERT for Text Generation\"](https://arxiv.org/abs/1911.03829) (ACL 2020):\n",
    "\n",
    "1. **CMLM Finetuning**: Fine-tune BERT as a conditional masked language model on translation data\n",
    "2. **Knowledge Extraction**: Extract hidden states and compute teacher logits\n",
    "3. **Student Training**: Train a smaller encoder-decoder translation model with knowledge distillation\n",
    "\n",
    "We'll work with the IWSLT14 German-English dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad931c70",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "\n",
    "First, we'll install the required dependencies and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8d11da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers==4.26.0\n",
    "!pip install pytorch-pretrained-bert\n",
    "!pip install cytoolz\n",
    "!pip install tqdm\n",
    "!pip install shelve-utils\n",
    "\n",
    "# Import common libraries\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import shelve\n",
    "import io\n",
    "import argparse\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "\n",
    "# Add project directories to path for importing modules\n",
    "sys.path.append('.')\n",
    "sys.path.append('./opennmt')\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeea0bbb",
   "metadata": {},
   "source": [
    "## Download and Preprocess the IWSLT14 Dataset\n",
    "\n",
    "We'll download the German-English translation dataset and prepare it for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6653ac46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories for data and outputs\n",
    "!mkdir -p data/\n",
    "!mkdir -p output/cmlm_model\n",
    "!mkdir -p output/bert_dump\n",
    "!mkdir -p output/kd-model/ckpt\n",
    "!mkdir -p output/kd-model/log\n",
    "!mkdir -p output/translation\n",
    "\n",
    "# Download IWSLT German-English dataset using the provided script\n",
    "!bash scripts/download-iwslt_deen.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be62e15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset using the provided script\n",
    "!bash scripts/prepare-iwslt_deen.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbefae33",
   "metadata": {},
   "source": [
    "## Apply BERT Tokenization\n",
    "\n",
    "We need to tokenize our data with the BERT tokenizer for the CMLM finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8c4bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.bert_tokenize import tokenize, process\n",
    "\n",
    "# Load BERT tokenizer\n",
    "bert_model = \"bert-base-multilingual-cased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model, do_lower_case='uncased' in bert_model)\n",
    "\n",
    "# Define data directories\n",
    "data_dir = \"data/de-en\"\n",
    "\n",
    "# BERT tokenize our dataset files\n",
    "for language in ['de', 'en']:\n",
    "    for split in ['train', 'valid', 'test']:\n",
    "        input_file = f\"{data_dir}/{split}.{language}\"\n",
    "        output_file = f\"{data_dir}/{split}.{language}.bert\"\n",
    "        print(f\"Tokenizing {input_file}...\")\n",
    "        \n",
    "        with open(input_file, 'r') as reader, open(output_file, 'w') as writer:\n",
    "            process(reader, writer, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66af4716",
   "metadata": {},
   "source": [
    "## Prepare BERT Training Data\n",
    "\n",
    "Now we'll prepare the database and vocabulary for BERT finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a619e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset DB for BERT training\n",
    "from scripts.bert_prepro import main as bert_prepro\n",
    "\n",
    "# Set up args for bert_prepro\n",
    "prepro_args = argparse.Namespace(\n",
    "    src=f\"{data_dir}/train.de.bert\",\n",
    "    tgt=f\"{data_dir}/train.en.bert\",\n",
    "    output='data/DEEN.db'\n",
    ")\n",
    "\n",
    "# Run preprocessing\n",
    "bert_prepro(prepro_args)\n",
    "\n",
    "# Create vocabulary file using OpenNMT's preprocess.py\n",
    "!python opennmt/preprocess.py \\\n",
    "    -train_src {data_dir}/train.de.bert \\\n",
    "    -train_tgt {data_dir}/train.en.bert \\\n",
    "    -valid_src {data_dir}/valid.de.bert \\\n",
    "    -valid_tgt {data_dir}/valid.en.bert \\\n",
    "    -save_data data/DEEN \\\n",
    "    -src_seq_length 150 -tgt_seq_length 150"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c72ac6",
   "metadata": {},
   "source": [
    "## Stage 1: CMLM (Conditional Masked Language Model) Finetuning\n",
    "\n",
    "In this stage, we fine-tune BERT as a Conditional Masked Language Model on our translation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261c988b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert.optimization import BertAdam\n",
    "\n",
    "# Import needed modules\n",
    "from cmlm.data import BertDataset, TokenBucketSampler\n",
    "from cmlm.model import convert_embedding, BertForSeq2seq\n",
    "from cmlm.util import Logger, RunningMeter\n",
    "from run_cmlm_finetuning import noam_schedule\n",
    "\n",
    "# Load vocabulary\n",
    "vocab_file = \"data/DEEN.vocab.pt\"\n",
    "train_file = \"data/DEEN.db\"\n",
    "valid_src = f\"{data_dir}/valid.de.bert\"\n",
    "valid_tgt = f\"{data_dir}/valid.en.bert\"\n",
    "output_dir = \"output/cmlm_model\"\n",
    "\n",
    "# Load vocabulary\n",
    "vocab_dump = torch.load(vocab_file)\n",
    "vocab = vocab_dump['tgt'].fields[0][1].vocab.stoi\n",
    "\n",
    "# Create dataset\n",
    "train_dataset = BertDataset(train_file, tokenizer, vocab, seq_len=512, max_len=150)\n",
    "\n",
    "# Define sampler and data loader\n",
    "BUCKET_SIZE = 8192\n",
    "train_sampler = TokenBucketSampler(\n",
    "    train_dataset.lens, BUCKET_SIZE, 6144, batch_multiple=1)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_sampler=train_sampler,\n",
    "                         num_workers=4,\n",
    "                         collate_fn=BertDataset.pad_collate)\n",
    "\n",
    "# Prepare model\n",
    "model = BertForSeq2seq.from_pretrained(bert_model)\n",
    "embedding = convert_embedding(tokenizer, vocab, model.bert.embeddings.word_embeddings.weight)\n",
    "model.update_output_layer(embedding)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3157f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "learning_rate = 5e-5\n",
    "warmup_steps = 4000\n",
    "max_steps = 100000  # Full training uses 100k steps\n",
    "num_steps_to_run = 5000  # We'll do fewer steps for demonstration\n",
    "\n",
    "# Optimizer\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer\n",
    "                if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer\n",
    "                if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                    lr=learning_rate,\n",
    "                    warmup=warmup_steps,\n",
    "                    t_total=max_steps)\n",
    "\n",
    "# Training loop\n",
    "running_loss = RunningMeter('loss')\n",
    "model.train()\n",
    "\n",
    "print(\"Starting CMLM fine-tuning...\")\n",
    "for step, batch in enumerate(tqdm(train_loader, desc=\"Training\")):\n",
    "    if step >= num_steps_to_run:\n",
    "        break\n",
    "        \n",
    "    # Move batch to device\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    input_ids, input_mask, segment_ids, lm_label_ids = batch\n",
    "    \n",
    "    # Zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    loss = model(input_ids, segment_ids, input_mask, lm_label_ids)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    running_loss(loss.item())\n",
    "    \n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step}, Loss: {running_loss.val:.4f}\")\n",
    "\n",
    "# Save model checkpoint\n",
    "torch.save(model.state_dict(), f\"{output_dir}/model_step_{num_steps_to_run}.pt\")\n",
    "print(f\"Model saved to {output_dir}/model_step_{num_steps_to_run}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4981bfac",
   "metadata": {},
   "source": [
    "## Stage 2: Extract Knowledge from Teacher Model\n",
    "\n",
    "Now we'll extract the hidden states from our fine-tuned BERT model and compute the top-k logits that will be used for knowledge distillation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebefcaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import extraction functions\n",
    "from dump_teacher_hiddens import tensor_dumps, gather_hiddens, BertSampleDataset, batch_features, process_batch\n",
    "\n",
    "# Path to model checkpoint from Stage 1\n",
    "ckpt_path = f\"{output_dir}/model_step_{num_steps_to_run}.pt\"\n",
    "bert_dump_path = \"output/bert_dump\"\n",
    "\n",
    "# Load the fine-tuned BERT model\n",
    "state_dict = torch.load(ckpt_path)\n",
    "vsize = state_dict['cls.predictions.decoder.weight'].size(0)\n",
    "bert = BertForSeq2seq.from_pretrained(bert_model).eval()\n",
    "bert.to(device)\n",
    "\n",
    "# Update output layer\n",
    "bert.update_output_layer_by_size(vsize)\n",
    "bert.load_state_dict(state_dict)\n",
    "\n",
    "# Save the final projection layer\n",
    "linear = torch.nn.Linear(bert.config.hidden_size, bert.config.vocab_size)\n",
    "linear.weight.data = state_dict['cls.predictions.decoder.weight']\n",
    "linear.bias.data = state_dict['cls.predictions.bias']\n",
    "torch.save(linear, f'{bert_dump_path}/linear.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c58071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract hidden states\n",
    "def build_db_batched(corpus_path, out_db, bert, toker, batch_size=8):\n",
    "    dataset = BertSampleDataset(corpus_path, toker)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size,\n",
    "                       num_workers=4, collate_fn=batch_features)\n",
    "    \n",
    "    with tqdm(desc='Computing BERT features', total=len(dataset)) as pbar:\n",
    "        for ids, *batch in loader:\n",
    "            outputs = process_batch(batch, bert, toker)\n",
    "            for id_, output in zip(ids, outputs):\n",
    "                if output is not None:\n",
    "                    out_db[id_] = tensor_dumps(output)\n",
    "            pbar.update(len(ids))\n",
    "\n",
    "# Extract hidden states\n",
    "db_path = \"data/DEEN.db\"\n",
    "print(\"Extracting hidden states...\")\n",
    "with shelve.open(f'{bert_dump_path}/db', 'c') as out_db, torch.no_grad():\n",
    "    build_db_batched(db_path, out_db, bert, tokenizer, batch_size=8)\n",
    "\n",
    "print(f\"Hidden states extracted and saved to {bert_dump_path}/db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f31547",
   "metadata": {},
   "source": [
    "## Computing Top-K Logits\n",
    "\n",
    "Now we'll compute the top-k logits from the extracted hidden states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab6b179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions for top-k computation\n",
    "from dump_teacher_topk import tensor_loads, dump_topk\n",
    "\n",
    "# Top-K parameter\n",
    "k = 8  # Following the paper\n",
    "\n",
    "# Load linear layer\n",
    "linear = torch.load(f'{bert_dump_path}/linear.pt')\n",
    "linear.to(device)\n",
    "\n",
    "# Compute top-k logits\n",
    "print(\"Computing top-k logits...\")\n",
    "with shelve.open(f'{bert_dump_path}/db', 'r') as db, \\\n",
    "     shelve.open(f'{bert_dump_path}/topk', 'c') as topk_db:\n",
    "    for key, value in tqdm(db.items(), total=len(db), desc='Computing topk...'):\n",
    "        bert_hidden = torch.tensor(tensor_loads(value)).to(device)\n",
    "        topk = linear(bert_hidden).topk(dim=-1, k=k)\n",
    "        dump = dump_topk(topk)\n",
    "        topk_db[key] = dump\n",
    "\n",
    "print(f\"Top-k logits computed and saved to {bert_dump_path}/topk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9f07c7",
   "metadata": {},
   "source": [
    "## Stage 3: Train Student Translation Model with Knowledge Distillation\n",
    "\n",
    "Finally, we'll train a transformer-based translation model using the knowledge distilled from BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc88751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules for training\n",
    "from onmt.inputters.bert_kd_dataset import BertKdDataset, TokenBucketSampler\n",
    "from onmt.utils.optimizers import Optimizer\n",
    "from onmt.train_single import build_model_saver, build_trainer, cycle_loader\n",
    "\n",
    "# Define paths\n",
    "data_db = \"data/DEEN.db\"\n",
    "bert_dump = \"output/bert_dump\"\n",
    "data = \"data/DEEN\"\n",
    "config_path = \"opennmt/config/config-transformer-base-mt-deen.yml\"\n",
    "output_path = \"output/kd-model\"\n",
    "\n",
    "# Load configuration\n",
    "with open(config_path, 'r') as stream:\n",
    "    config = yaml.safe_load(stream)\n",
    "\n",
    "# Create args object\n",
    "args = argparse.Namespace(**config)\n",
    "\n",
    "# Setup KD parameters\n",
    "args.train_from = None\n",
    "args.max_grad_norm = None\n",
    "args.kd_topk = 8\n",
    "args.train_steps = 100000\n",
    "args.kd_temperature = 10.0\n",
    "args.kd_alpha = 0.5\n",
    "args.warmup_steps = 8000\n",
    "args.learning_rate = 2.0\n",
    "args.bert_dump = bert_dump\n",
    "args.data_db = data_db\n",
    "args.bert_kd = True\n",
    "args.data = data\n",
    "\n",
    "args.save_model = os.path.join(output_path, 'ckpt', 'model')\n",
    "args.log_file = os.path.join(output_path, 'log', 'log')\n",
    "args.tensorboard_log_dir = os.path.join(output_path, 'log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8247096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vocabulary and dataset\n",
    "vocab = torch.load(data + '.vocab.pt')\n",
    "src_vocab = vocab['src'].fields[0][1].vocab.stoi\n",
    "tgt_vocab = vocab['tgt'].fields[0][1].vocab.stoi\n",
    "\n",
    "# Create dataset\n",
    "train_dataset = BertKdDataset(data_db, bert_dump, \n",
    "                             src_vocab, tgt_vocab,\n",
    "                             max_len=150, k=args.kd_topk)\n",
    "\n",
    "# Create data loader\n",
    "BUCKET_SIZE = 8192\n",
    "train_sampler = TokenBucketSampler(\n",
    "    train_dataset.keys, BUCKET_SIZE, 6144,\n",
    "    batch_multiple=1)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_sampler=train_sampler,\n",
    "                         num_workers=4,\n",
    "                         collate_fn=BertKdDataset.pad_collate)\n",
    "\n",
    "train_iter = cycle_loader(train_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87617e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "from onmt.model_builder import build_model\n",
    "model = build_model(args, args, checkpoint=None)\n",
    "model.to(device)\n",
    "\n",
    "# Build optimizer\n",
    "optim = Optimizer.from_opt(model, args, checkpoint=None)\n",
    "\n",
    "# Build model saver\n",
    "model_saver = build_model_saver(args, args, model, vocab, optim)\n",
    "\n",
    "# Build trainer\n",
    "trainer = build_trainer(args, 0, model, vocab, optim, model_saver=model_saver)\n",
    "\n",
    "# Train - for demonstration, we'll only do a few steps\n",
    "num_steps_to_run_kd = 500  # Adjust for full training (paper used 100k steps)\n",
    "\n",
    "print(\"Starting model training with knowledge distillation...\")\n",
    "trainer.train(\n",
    "    train_iter,\n",
    "    num_steps_to_run_kd,\n",
    "    valid_iter=None\n",
    ")\n",
    "\n",
    "print(f\"Model trained for {num_steps_to_run_kd} steps and saved to {output_path}/ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e16cd0",
   "metadata": {},
   "source": [
    "## Translation and Evaluation\n",
    "\n",
    "Finally, we'll translate some text using our trained model and evaluate the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd939b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths for translation\n",
    "model_path = f\"{output_path}/ckpt/model_step_{num_steps_to_run_kd}.pt\"\n",
    "src_file = f\"{data_dir}/test.de.bert\"\n",
    "tgt_file = f\"{data_dir}/test.en.bert\"\n",
    "out_dir = \"output/translation\"\n",
    "ref_file = f\"{data_dir}/test.en\"\n",
    "\n",
    "# Run translation if model exists\n",
    "if os.path.exists(model_path):\n",
    "    # Run translation\n",
    "    !python opennmt/translate.py -model {model_path} \\\n",
    "                                -src {src_file} \\\n",
    "                                -tgt {tgt_file} \\\n",
    "                                -output {out_dir}/result.en \\\n",
    "                                -beam_size 5 -alpha 0.6 \\\n",
    "                                -length_penalty wu\n",
    "\n",
    "    # Detokenize output\n",
    "    !python scripts/bert_detokenize.py --file {out_dir}/result.en \\\n",
    "                                      --output_dir {out_dir}\n",
    "\n",
    "    # Evaluate with BLEU\n",
    "    !perl opennmt/tools/multi-bleu.perl {ref_file} \\\n",
    "                                       < {out_dir}/result.en.detok \\\n",
    "                                       > {out_dir}/result.bleu\n",
    "\n",
    "    # Display BLEU score\n",
    "    with open(f\"{out_dir}/result.bleu\", \"r\") as f:\n",
    "        print(f.read())\n",
    "else:\n",
    "    print(f\"Model file {model_path} not found. Skipping translation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f2ff3e",
   "metadata": {},
   "source": [
    "## Visualize Training Results\n",
    "\n",
    "Let's visualize the training progress and compare with the results from the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb07bf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install matplotlib if needed\n",
    "!pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Display the figures from the paper\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Plot CMLM finetuning\n",
    "axes[0].set_title('CMLM Finetuning')\n",
    "img = plt.imread('figures/cmlm-finetuning.png')\n",
    "axes[0].imshow(img)\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Plot translation losses\n",
    "axes[1].set_title('Translation Losses')\n",
    "img = plt.imread('figures/translation-losses.png')\n",
    "axes[1].imshow(img)\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Plot translation accuracy\n",
    "axes[2].set_title('Translation Accuracy')\n",
    "img = plt.imread('figures/translation-accuracy.png')\n",
    "axes[2].imshow(img)\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fc3506",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've implemented the three-stage knowledge distillation process described in the paper \"Distilling Knowledge Learned in BERT for Text Generation\":\n",
    "\n",
    "1. Fine-tuned a BERT model as a Conditional Masked Language Model (CMLM)\n",
    "2. Extracted knowledge from the BERT teacher model and computed top-k logits\n",
    "3. Trained a student translation model with knowledge distillation from BERT\n",
    "\n",
    "For a full implementation with complete results, the model should be trained for many more steps:\n",
    "- CMLM finetuning: 100,000 steps\n",
    "- Student model training: 100,000 steps\n",
    "\n",
    "As shown in the figures, knowledge distillation from BERT can improve translation performance compared to baseline methods."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
